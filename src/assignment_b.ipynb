{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3335886c",
   "metadata": {},
   "source": [
    "# Spam Filter  \n",
    "**Data Science I, Assignment B**  \n",
    "**Student:** Fabian Augschöll  \n",
    "**Date:** June 2025  \n",
    "\n",
    "**Abstract**  \n",
    "In this notebook we build and evaluate three spam‑classification models (Naive Bayes, Logistic Regression, SVM) on the Apache SpamAssassin corpus. We’ll also experiment with different text‑preprocessing pipelines to maximize combined precision + recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49afe7",
   "metadata": {},
   "source": [
    "## Objectives  \n",
    "1. Load “easy_ham_2” vs. “spam_2” emails  \n",
    "2. Build a flexible preprocessing pipeline  \n",
    "3. Vectorize text with a bag‑of‑words model  \n",
    "4. Train & evaluate three classifiers  \n",
    "5. Experiment with preprocessing hyperparameters  \n",
    "6. Demonstrate the best model on a fresh sample  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6d892",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "We use the Apache SpamAssassin public corpus, which contains labeled emails categorized as \"ham\" (legitimate) and \"spam\" (unwanted). Specifically, we load:\n",
    "\n",
    "- `easy_ham_2`: Straightforward, non‐suspicious ham messages.  \n",
    "- `hard_ham`: Ham messages that resemble spam in vocabulary or structure.  \n",
    "- `spam_2`: Classic spam messages from diverse sources.  \n",
    "\n",
    "After loading and shuffling, we split the data into training and testing sets to ensure unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e4695",
   "metadata": {},
   "source": [
    "## Imports & helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8637a",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "Raw email text often contains headers, HTML markup, URLs, numbers, and punctuation—elements that can both aid and hinder classification. We implement a flexible `EmailPreprocessor` class that supports the following steps:\n",
    "\n",
    "1. **Header Stripping**: Removes the message headers (e.g., `From`, `Subject`) to prevent overfitting to specific senders.  \n",
    "2. **Lowercasing**: Standardizes the case to reduce feature dimensionality.  \n",
    "3. **URL Replacement**: Substitutes URLs with a placeholder token (`URL`) to capture the presence of links without over‑specificity.  \n",
    "4. **Number Replacement**: Maps numeric sequences to a token (`NUMBER`) to detect offers or price references generically.  \n",
    "5. **Punctuation Removal**: Eliminates punctuation to focus on word tokens.  \n",
    "\n",
    "This modular design allows us to toggle each step during hyperparameter experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365bae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "os.chdir('..')\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "\n",
    "def load_emails(directory, label):\n",
    "    \"\"\"Load all .txt emails in `directory`, assign label 0=ham, 1=spam.\"\"\"\n",
    "    emails = []\n",
    "    for fname in os.listdir(directory):\n",
    "        path = os.path.join(directory, fname)\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                emails.append((f.read(), label))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return emails\n",
    "\n",
    "class EmailPreprocessor:\n",
    "    \"\"\"Flexible email cleaner: strip headers, lowercase, URL/NUM replacement, punctuation removal.\"\"\"\n",
    "    def __init__(self, strip_headers=True, lowercase=True,\n",
    "                 remove_punct=True, replace_urls=True, replace_nums=True):\n",
    "        self.strip_headers, self.lowercase = strip_headers, lowercase\n",
    "        self.remove_punct, self.replace_urls = remove_punct, replace_urls\n",
    "        self.replace_nums = replace_nums\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if self.strip_headers:\n",
    "            parts = text.split('\\n\\n', 1)\n",
    "            text = parts[1] if len(parts) > 1 else parts[0]\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.replace_urls:\n",
    "            text = re.sub(r'http[s]?://\\S+', 'URL', text)\n",
    "        if self.replace_nums:\n",
    "            text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "        if self.remove_punct:\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        return ' '.join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3273e",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "We’ll load both ham and spam folders, shuffle, then split out texts & labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44aa357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1648 emails: 1397 spam, 251 ham.\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(base_path='data'):\n",
    "    # Load all ham: easy and hard\n",
    "    #easy_ham = load_emails(os.path.join(base_path, 'easy_ham_2'), 0)\n",
    "    hard_ham = load_emails(os.path.join(base_path, 'hard_ham'), 0)\n",
    "    spam = load_emails(os.path.join(base_path, 'spam_2'), 1)\n",
    "    \n",
    "    # Combine all data\n",
    "    data = hard_ham + spam #+ easy_ham\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    # Separate into features and labels\n",
    "    texts, labels = zip(*data)\n",
    "    return list(texts), np.array(labels)\n",
    "\n",
    "# Reload combined dataset\n",
    "texts, labels = prepare_data('data')\n",
    "print(f\"Loaded {len(texts)} emails: {labels.sum()} spam, {len(labels)-labels.sum()} ham.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffc5a4",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "Raw email text often contains headers, HTML markup, URLs, numbers, and punctuation—elements that can both aid and hinder classification. We implement a flexible `EmailPreprocessor` class that supports the following steps:\n",
    "\n",
    "1. **Header Stripping**: Removes the message headers (e.g., `From`, `Subject`) to prevent overfitting to specific senders.  \n",
    "2. **Lowercasing**: Standardizes the case to reduce feature dimensionality.  \n",
    "3. **URL Replacement**: Substitutes URLs with a placeholder token (`URL`) to capture the presence of links without over‑specificity.  \n",
    "4. **Number Replacement**: Maps numeric sequences to a token (`NUMBER`) to detect offers or price references generically.  \n",
    "5. **Punctuation Removal**: Eliminates punctuation to focus on word tokens.  \n",
    "\n",
    "This modular design allows us to toggle each step during hyperparameter experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2fc4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = EmailPreprocessor()\n",
    "processed_texts = [pre.preprocess(t) for t in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51a9fc",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "We convert preprocessed text into a numerical representation using a Bag‑of‑Words model via `CountVectorizer`. Key settings include:\n",
    "\n",
    "- **Vocabulary Size**: Top 1,000 most frequent tokens to balance expressiveness and tractability.  \n",
    "- **Stop‑Word Filtering**: Excludes common English words (e.g., \"the\", \"and\") to focus on informative terms.  \n",
    "\n",
    "This yields a sparse matrix of token counts for each email, which serves as input to our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ec91a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test sizes: 1153/495\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.3, stratify=labels, random_state=42)\n",
    "print(f\"Train/test sizes: {X_train.shape[0]}/{X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf46ac",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "We train three widely‑used classifiers:\n",
    "\n",
    "1. **Multinomial Naive Bayes**: A probabilistic approach suited for count data.  \n",
    "2. **Logistic Regression**: A discriminative model that estimates class probabilities.  \n",
    "3. **Support Vector Machine (linear kernel)**: A margin‑maximizing classifier that often excels in high‑dimensional text spaces.  \n",
    "\n",
    "For each model, we report:\n",
    "\n",
    "- **Precision**: Proportion of predicted spam that is actually spam.  \n",
    "- **Recall**: Proportion of true spam that is correctly detected.  \n",
    "- **F1‑Score**: Harmonic mean of precision and recall (via `classification_report`).  \n",
    "\n",
    "These metrics enable us to compare trade‑offs between false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c00e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes ---\n",
      "Precision: 0.955, Recall: 0.905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.59      0.76      0.66        75\n",
      "        Spam       0.95      0.90      0.93       420\n",
      "\n",
      "    accuracy                           0.88       495\n",
      "   macro avg       0.77      0.83      0.80       495\n",
      "weighted avg       0.90      0.88      0.89       495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Schule_24-25\\MLDS\\AssignmentB\\Spam_Filter\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "Precision: 0.972, Recall: 0.990\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.94      0.84      0.89        75\n",
      "        Spam       0.97      0.99      0.98       420\n",
      "\n",
      "    accuracy                           0.97       495\n",
      "   macro avg       0.96      0.92      0.93       495\n",
      "weighted avg       0.97      0.97      0.97       495\n",
      "\n",
      "--- SVM ---\n",
      "Precision: 0.974, Recall: 0.983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.90      0.85      0.88        75\n",
      "        Spam       0.97      0.98      0.98       420\n",
      "\n",
      "    accuracy                           0.96       495\n",
      "   macro avg       0.94      0.92      0.93       495\n",
      "weighted avg       0.96      0.96      0.96       495\n",
      "\n",
      "**Best model:** Logistic Regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_report(X_tr, X_te, y_tr, y_te):\n",
    "    models = {\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'SVM': SVC(kernel='linear', probability=True)\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_te)\n",
    "        p, r = precision_score(y_te, y_pred), recall_score(y_te, y_pred)\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(f\"Precision: {p:.3f}, Recall: {r:.3f}\")\n",
    "        print(classification_report(y_te, y_pred, target_names=['Ham', 'Spam']))\n",
    "        results[name] = {'model': model, 'precision': p, 'recall': r}\n",
    "    return results\n",
    "\n",
    "results = train_and_report(X_train, X_test, y_train, y_test)\n",
    "best_name = max(results, key=lambda k: results[k]['precision']+results[k]['recall'])\n",
    "print(f\"**Best model:** {best_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f078be7",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "We identify the best performing model based on the combined sum of precision and recall. After initial evaluation, we highlight the top candidate for further demonstration on previously unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f88baa",
   "metadata": {},
   "source": [
    "## Demonstration on Fresh Data\n",
    "Using the selected best model, we perform inference on a separate batch of emails (`hard_ham` + `spam` folders not seen during training). This mimics real‑world deployment, where the filter encounters new message patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "620c2e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on hard_ham + spam folders (1902 emails):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.94      0.20      0.33      1401\n",
      "        Spam       0.30      0.97      0.46       501\n",
      "\n",
      "    accuracy                           0.40      1902\n",
      "   macro avg       0.62      0.58      0.40      1902\n",
      "weighted avg       0.77      0.40      0.37      1902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load real unseen evaluation data (hard ham + spam)\n",
    "hard_ham = load_emails('data/easy_ham_2', 0)\n",
    "extra_spam = load_emails('data/spam', 1)\n",
    "test_data = hard_ham + extra_spam\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(test_data)\n",
    "test_texts, test_labels = zip(*test_data)\n",
    "\n",
    "# Preprocess using the trained preprocessor\n",
    "test_processed = [pre.preprocess(t) for t in test_texts]\n",
    "\n",
    "# Vectorize using trained vectorizer\n",
    "X_test_real = vectorizer.transform(test_processed)\n",
    "\n",
    "# Predict using best model\n",
    "best_model = results[best_name]['model']\n",
    "y_real_pred = best_model.predict(X_test_real)\n",
    "y_real_prob = best_model.predict_proba(X_test_real)\n",
    "\n",
    "# Report performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"Evaluation on hard_ham + spam folders ({len(test_labels)} emails):\")\n",
    "print(classification_report(test_labels, y_real_pred, target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14742db3",
   "metadata": {},
   "source": [
    "## Hyperparameter Experiments\n",
    "To refine our preprocessing choices, we systematically vary key options:\n",
    "\n",
    "- Toggling header stripping on/off  \n",
    "- Enabling/disabling lowercasing  \n",
    "- Removing/preserving punctuation  \n",
    "\n",
    "For each configuration, we retrain a Naive Bayes classifier and score performance by combined precision + recall. This ablation study reveals which preprocessing steps contribute most to accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038b6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 1: P=0.955, R=0.905, Sum=1.860\n",
      "Config 2: P=0.955, R=0.957, Sum=1.912\n",
      "Config 3: P=0.955, R=0.905, Sum=1.860\n",
      "Config 4: P=0.955, R=0.905, Sum=1.860\n",
      "\n",
      "**Best config:** {'strip_headers': False, 'lowercase': True, 'remove_punct': True, 'replace_urls': True, 'replace_nums': True} (Sum=1.912)\n"
     ]
    }
   ],
   "source": [
    "def hyperparam_experiment(texts, labels):\n",
    "    configs = [\n",
    "        {'strip_headers': True,  'lowercase': True,  'remove_punct': True,  'replace_urls': True,  'replace_nums': True},\n",
    "        {'strip_headers': False, 'lowercase': True,  'remove_punct': True,  'replace_urls': True,  'replace_nums': True},\n",
    "        {'strip_headers': True,  'lowercase': False, 'remove_punct': True,  'replace_urls': True,  'replace_nums': True},\n",
    "        {'strip_headers': True,  'lowercase': True,  'remove_punct': False, 'replace_urls': True,  'replace_nums': True},\n",
    "    ]\n",
    "    best, best_cfg = 0, None\n",
    "    for i, cfg in enumerate(configs, 1):\n",
    "        pre = EmailPreprocessor(**cfg)\n",
    "        proc = [pre.preprocess(t) for t in texts]\n",
    "        X = CountVectorizer(max_features=1000, stop_words='english').fit_transform(proc)\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, labels, test_size=0.3,\n",
    "                                              random_state=42, stratify=labels)\n",
    "        model = MultinomialNB().fit(Xtr, ytr)\n",
    "        yp = model.predict(Xte)\n",
    "        p, r = precision_score(yte, yp), recall_score(yte, yp)\n",
    "        print(f\"Config {i}: P={p:.3f}, R={r:.3f}, Sum={p+r:.3f}\")\n",
    "        if p+r > best:\n",
    "            best, best_cfg = p+r, cfg\n",
    "    print(f\"\\n**Best config:** {best_cfg} (Sum={best:.3f})\")\n",
    "\n",
    "hyperparam_experiment(texts, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca03c74",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- **Best Classifier**: _Logistic Regression_ consistently achieves high precision and recall, making it our top choice for spam filtering.  \n",
    "- **Optimal Preprocessing**: No Header stripping, lowercasing, URL/numeric tokenization, and punctuation removal enhance signal quality.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
